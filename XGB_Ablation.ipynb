{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca(X, components = 3):\n",
    "    '''\n",
    "    first = True\n",
    "    for images in X:\n",
    "        if first:\n",
    "            train_for_pca = images\n",
    "            first = False\n",
    "        else:\n",
    "            # print(train_for_pca.shape)\n",
    "            train_for_pca = np.concatenate([train_for_pca, images])    \n",
    "    '''\n",
    "    train_for_pca = X\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_for_pca)\n",
    "    train_for_pca=scaler.transform(train_for_pca)\n",
    "    pca = PCA(n_components = components)\n",
    "    pca.fit(train_for_pca)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensional_reduction_ablation(X, pca):\n",
    "    reduced_X = []\n",
    "    reduced_X_mean = []\n",
    "    reduced_X_std = []\n",
    "    reduced_X_len = []\n",
    "    reduced_X_rho = []\n",
    "    \n",
    "    for images in X:\n",
    "        train_pca = pca.transform(images)\n",
    "        # embedded statistics\n",
    "        train_x = np.append(np.concatenate([np.mean(train_pca, axis = 0), np.std(train_pca, axis = 0)]), len(images))\n",
    "        train_x_mean = np.append(np.std(train_pca, axis = 0), len(images))\n",
    "        train_x_std = np.append(np.mean(train_pca, axis = 0), len(images))\n",
    "        train_x_len = np.concatenate([np.mean(train_pca, axis = 0), np.std(train_pca, axis = 0)])\n",
    "        train_x_rho = np.append(np.concatenate([np.mean(train_pca, axis = 0), np.std(train_pca, axis = 0)]), len(images))\n",
    "        \n",
    "        # coor\n",
    "        colnum = train_pca.shape[1]\n",
    "        for subset in itertools.combinations(range(colnum), 2):\n",
    "            train_x = np.append(train_x, np.corrcoef(train_pca[:, subset[0]], train_pca[:, subset[1]])[0][1])\n",
    "            train_x_mean = np.append(train_x_mean, np.corrcoef(train_pca[:, subset[0]], train_pca[:, subset[1]])[0][1])\n",
    "            train_x_std = np.append(train_x_std, np.corrcoef(train_pca[:, subset[0]], train_pca[:, subset[1]])[0][1])\n",
    "            train_x_len = np.append(train_x_len, np.corrcoef(train_pca[:, subset[0]], train_pca[:, subset[1]])[0][1])\n",
    "        \n",
    "        #instance\n",
    "        reduced_X.append(train_x)\n",
    "        reduced_X_mean.append(train_x_mean)\n",
    "        reduced_X_std.append(train_x_std)\n",
    "        reduced_X_len.append(train_x_len)\n",
    "        reduced_X_rho.append(train_x_rho)\n",
    "        \n",
    "    reduced_X = np.array(reduced_X)\n",
    "    reduced_X_mean = np.array(reduced_X_mean)\n",
    "    reduced_X_std = np.array(reduced_X_std)\n",
    "    reduced_X_len = np.array(reduced_X_len)\n",
    "    reduced_X_rho = np.array(reduced_X_rho)\n",
    "    \n",
    "    return reduced_X, reduced_X_mean, reduced_X_std, reduced_X_len, reduced_X_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, fold = 4, seed = 42):\n",
    "    regr = xgb.XGBRegressor(objective=\"reg:linear\", random_state=seed)\n",
    "    \n",
    "    # k-fold\n",
    "    kf = KFold(n_splits = 4, shuffle = True, random_state = seed)\n",
    "    scores = []\n",
    "    for train_ids, valid_ids in kf.split(X):\n",
    "        trainX = X[train_ids]; trainY = y[train_ids]\n",
    "        validX = X[valid_ids]; validY = y[valid_ids]\n",
    "        \n",
    "        regr.fit(trainX, trainY)\n",
    "        y_pred = regr.predict(validX)\n",
    "        r2_val = r2_score(validY, y_pred)\n",
    "        scores.append(r2_val)\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_res_ablation_both(dimension, X, X1, y, y1, PAC, PAC1):\n",
    "    X_ = np.concatenate(X)\n",
    "    \n",
    "    # 训练降维器\n",
    "    pca = train_pca(X_, components = dimension)\n",
    "    \n",
    "    # 降维并提取特征\n",
    "    reduced_trainX, reduced_trainX_mean, reduced_trainX_std, reduced_trainX_len, reduced_trainX_rho = dimensional_reduction_ablation(X, pca)\n",
    "    reduced_validX, reduced_validX_mean, reduced_validX_std, reduced_validX_len, reduced_validX_rho = dimensional_reduction_ablation(X1, pca)\n",
    "    trainSet = {'whole': reduced_trainX,\n",
    "               'mean': reduced_trainX_mean,\n",
    "               'std': reduced_trainX_std,\n",
    "               'len': reduced_trainX_len,\n",
    "               'rho': reduced_trainX_rho}\n",
    "    validSet = {'whole': reduced_validX,\n",
    "               'mean': reduced_validX_mean,\n",
    "               'std': reduced_validX_std,\n",
    "               'len': reduced_validX_len,\n",
    "               'rho': reduced_validX_rho}\n",
    "\n",
    "    ### 去掉空值\n",
    "    reduced_trainSet = {}\n",
    "    reduced_validSet = {}\n",
    "    # 针对每一种情况分别去空值\n",
    "    for cls in trainSet.keys():\n",
    "        yt = np.delete(y, np.where(~np.isnan(trainSet[cls]).any(axis=1) == False))\n",
    "        y1v = np.delete(y1, np.where(~np.isnan(validSet[cls]).any(axis=1) == False))\n",
    "        PACt = np.delete(PAC, np.where(~np.isnan(trainSet[cls]).any(axis=1) == False))\n",
    "        PAC1v = np.delete(PAC1, np.where(~np.isnan(validSet[cls]).any(axis=1) == False))\n",
    "        trainX = (trainSet[cls])[~np.isnan(trainSet[cls]).any(axis=1), :]\n",
    "        validX = (validSet[cls])[~np.isnan(validSet[cls]).any(axis=1), :]\n",
    "        \n",
    "        reduced_trainSet[cls] = [trainX, yt, PACt]\n",
    "        reduced_validSet[cls] = [validX, y1v, PAC1v]\n",
    "        \n",
    "    \n",
    "    return reduced_trainSet, reduced_validSet\n",
    "\n",
    "def pca_res_ablation(dimension, X, y, PAC):\n",
    "    X_ = np.concatenate(X)\n",
    "    \n",
    "    # 训练降维器\n",
    "    pca = train_pca(X_, components = dimension)\n",
    "    \n",
    "    # 降维并提取特征\n",
    "    reduced_trainX, reduced_trainX_mean, reduced_trainX_std, reduced_trainX_len, reduced_trainX_rho = dimensional_reduction_ablation(X, pca)\n",
    "    trainSet = {'whole': reduced_trainX,\n",
    "               'mean': reduced_trainX_mean,\n",
    "               'std': reduced_trainX_std,\n",
    "               'len': reduced_trainX_len,\n",
    "               'rho': reduced_trainX_rho}\n",
    "\n",
    "    ### 去掉空值\n",
    "    reduced_trainSet = {}\n",
    "    # 针对每一种情况分别去空值\n",
    "    for cls in trainSet.keys():\n",
    "        yt = np.delete(y, np.where(~np.isnan(trainSet[cls]).any(axis=1) == False))\n",
    "        PACt = np.delete(PAC, np.where(~np.isnan(trainSet[cls]).any(axis=1) == False))\n",
    "        trainX = (trainSet[cls])[~np.isnan(trainSet[cls]).any(axis=1), :]\n",
    "        \n",
    "        reduced_trainSet[cls] = [trainX, yt, PACt]\n",
    "        \n",
    "    \n",
    "    return reduced_trainSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入原始数据\n",
    "data = pd.read_csv('Data/2017_features.csv')\n",
    "data = data.dropna()\n",
    "#data = data[data['features']!='adsadas']\n",
    "label = pd.read_csv('Data/PAC_GDP17.csv')\n",
    "data = pd.merge(data, label, how='left')\n",
    "X = []\n",
    "y = []\n",
    "PAC = []\n",
    "for i in data.index:\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    #try:\n",
    "    x_i = [float(x) for x in re.split(r', |\\[|\\]', data['features'].loc[i]) if len(x) > 0]\n",
    "    x_i = np.array(x_i).reshape(-1, 4096)\n",
    "    X.append(x_i)\n",
    "    y.append(data['GDP'].loc[i])\n",
    "    PAC.append(data['PAC'].loc[i])\n",
    "    #except:\n",
    "       # print('error')\n",
    "zeros = [X.index(x) for x in X if x.shape[0] == 0]\n",
    "X = np.delete(np.array(X), zeros)\n",
    "y = np.delete(np.array(y), zeros)\n",
    "PAC = np.delete(np.array(PAC), zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the Optimal Dimension of PCA via K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "pca_range = [i for i in range(10, 26, 1)]\n",
    "for d in pca_range:\n",
    "    # 对每一个维度降维\n",
    "    print('==========', d, '==========')\n",
    "    cur_res = []\n",
    "    reduced_trainSet = pca_res_ablation(d, X, y, PAC)\n",
    "    \n",
    "    # 对每一种统计量计算方法\n",
    "    for cls in reduced_trainSet.keys():\n",
    "        score = linear_regression(reduced_trainSet[cls][0], np.log(reduced_trainSet[cls][1]))\n",
    "        cur_res.append(score)\n",
    "        print(cls, score)\n",
    "        \n",
    "    all_res.append(cur_res)   \n",
    "    print('dimension:', d, ' results:', cur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = np.array(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for the Optimal Parameters of XGBboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入2018年数据\n",
    "data1 = pd.read_csv('Data/2018_features.csv')\n",
    "data1 = data1.dropna()\n",
    "label1 = pd.read_csv('Data/PAC_GDP18.csv')\n",
    "data1 = pd.merge(data1, label1, how='left')\n",
    "X1 = []\n",
    "y1 = []\n",
    "PAC1 = []\n",
    "for i in data1.index:\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    #try:\n",
    "    x_i = [float(x) for x in re.split(r', |\\[|\\]', data1['features'].loc[i]) if len(x) > 0]\n",
    "    x_i = np.array(x_i).reshape(-1, 4096)\n",
    "    X1.append(x_i)\n",
    "    y1.append(data1['GDP'].loc[i])\n",
    "    PAC1.append(data1['PAC'].loc[i])\n",
    "    #except:\n",
    "       # print('error')\n",
    "zeros1 = [X1.index(x) for x in X1 if x.shape[0] == 0]\n",
    "X1 = np.delete(np.array(X1), zeros1)\n",
    "y1 = np.delete(np.array(y1), zeros1)\n",
    "PAC1 = np.delete(np.array(PAC1), zeros1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### search for the optimal parameters for each condition respectively\n",
    "\n",
    "# just a sample\n",
    "pca_opt = 18 # sample\n",
    "reduced_trainSet, reduced_validSet = pca_res_ablation_both(pca_opt, X, X1, y, y1, PAC, PAC1)\n",
    "\n",
    "# just a sample\n",
    "tX_mean, tY_mean, tPAC_mean = reduced_trainSet['mean']\n",
    "vX_mean, vY_mean, vPAC_mean = reduced_validSet['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a sample for tuning\n",
    "tree_params = {'objective': 'reg:linear', 'booster': 'gbtree', 'random_state': 42}\n",
    "cv_params = {'eta': np.arange(0.1, 10, 3),\n",
    "             'subsample': np.arange(0.5, 1, 0.2),\n",
    "            'lambda': np.arange(1, 100, 30),\n",
    "             'min_child_weight': np.arange(1, 100, 30)}\n",
    "model = xgb.XGBRegressor(**tree_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='r2', cv=4, verbose=1)\n",
    "optimized_GBM.fit(tX_mean, np.log(tY_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print('参数的最佳取值：{0}'.format(optimized_GBM.best_params_))\n",
    "print('最佳模型得分:{0}'.format(optimized_GBM.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### interate the tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters in our experiments\n",
    "\n",
    "# ablation mean\n",
    "# pca = 18\n",
    "opt_params_mean = {'objective': 'reg:linear', 'booster': 'gbtree', 'random_state': 42,\n",
    "              'subsample': 0.7, 'min_child_weight': 30, 'eta': 0.07, 'lambda': 3.5}\n",
    "opt_model_mean = xgb.XGBRegressor(**opt_params_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model_mean.fit(tX_mean, np.log(tY_mean))\n",
    "y_pred_mean = opt_model_mean.predict(vX_mean)\n",
    "r2_score(np.log(vY_mean), y_pred_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
