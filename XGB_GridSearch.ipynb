{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca(X, components = 3):\n",
    "    '''\n",
    "    first = True\n",
    "    for images in X:\n",
    "        if first:\n",
    "            train_for_pca = images\n",
    "            first = False\n",
    "        else:\n",
    "            # print(train_for_pca.shape)\n",
    "            train_for_pca = np.concatenate([train_for_pca, images])    \n",
    "    '''\n",
    "    train_for_pca = X\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_for_pca)\n",
    "    train_for_pca=scaler.transform(train_for_pca)\n",
    "    pca = PCA(n_components = components)\n",
    "    pca.fit(train_for_pca)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensional_reduction(X, pca):\n",
    "    reduced_X = []\n",
    "    for images in X:\n",
    "        train_pca = pca.transform(images)\n",
    "        train_x = np.append(np.concatenate([np.mean(train_pca, axis = 0), np.std(train_pca, axis = 0)]), len(images))\n",
    "        colnum = train_pca.shape[1]\n",
    "        for subset in itertools.combinations(range(colnum), 2):\n",
    "            train_x = np.append(train_x, np.corrcoef(train_pca[:, subset[0]], train_pca[:, subset[1]])[0][1])\n",
    "            \n",
    "        reduced_X.append(train_x)        \n",
    "    reduced_X = np.array(reduced_X)\n",
    "    return reduced_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, fold = 4, seed = 42):\n",
    "    regr = xgb.XGBRegressor(objective=\"reg:linear\", random_state=seed)\n",
    "    \n",
    "    # k折交叉\n",
    "    kf = KFold(n_splits = 4, shuffle = True, random_state=seed)\n",
    "    scores = []\n",
    "    for train_ids, valid_ids in kf.split(X):\n",
    "        trainX = X[train_ids]; trainY = y[train_ids]\n",
    "        validX = X[valid_ids]; validY = y[valid_ids]\n",
    "        \n",
    "        regr.fit(trainX, trainY)\n",
    "        y_pred = regr.predict(validX)\n",
    "        r2_val = r2_score(validY, y_pred)\n",
    "        scores.append(r2_val)\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [1, 2, 33, 3]\n",
    "temp.index(max(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入原始数据\n",
    "data = pd.read_csv('Data/2017_features.csv')\n",
    "data = data.dropna()\n",
    "#data = data[data['features']!='adsadas']\n",
    "label = pd.read_csv('Data/PAC_GDP17.csv')\n",
    "data = pd.merge(data, label, how='left')\n",
    "X = []\n",
    "y = []\n",
    "PAC = []\n",
    "for i in data.index:\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    #try:\n",
    "    x_i = [float(x) for x in re.split(r', |\\[|\\]', data['features'].loc[i]) if len(x) > 0]\n",
    "    x_i = np.array(x_i).reshape(-1, 4096)\n",
    "    X.append(x_i)\n",
    "    y.append(data['GDP'].loc[i])\n",
    "    PAC.append(data['PAC'].loc[i])\n",
    "    #except:\n",
    "       # print('error')\n",
    "zeros = [X.index(x) for x in X if x.shape[0] == 0]\n",
    "X = np.delete(np.array(X), zeros)\n",
    "y = np.delete(np.array(y), zeros)\n",
    "PAC = np.delete(np.array(PAC), zeros)\n",
    "X_ = np.concatenate(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the Optimal Dimension of PCA via K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores = []\n",
    "# 训练PCA降维器并提取特征\n",
    "pca_range = [i for i in range(3, 26, 1)]\n",
    "for dimension in pca_range:\n",
    "    print('=======', dimension, '=======')\n",
    "    \n",
    "    # 训练降维器\n",
    "    pca = train_pca(X_, components = dimension)\n",
    "    \n",
    "    # 降维并提取特征\n",
    "    reduced_trainX = dimensional_reduction(X, pca)\n",
    "    \n",
    "    # 去掉空值\n",
    "    yt = np.delete(y, np.where(~np.isnan(reduced_trainX).any(axis=1) == False))\n",
    "    PACt = np.delete(PAC, np.where(~np.isnan(reduced_trainX).any(axis=1) == False))\n",
    "    reduced_trainX = reduced_trainX[~np.isnan(reduced_trainX).any(axis=1), :]\n",
    "    \n",
    "    # 计算r2\n",
    "    r2 = linear_regression(reduced_trainX, np.log(yt))\n",
    "    r2_scores.append(r2)\n",
    "    print('r2:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for the Optimal Parameters of XGBboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_opt = pca_range[r2_scores.index(max(r2_scores))] # optimal value in our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练降维器\n",
    "pca = train_pca(X_, components = pca_opt)\n",
    "    \n",
    "# 降维并提取特征\n",
    "reduced_trainX = dimensional_reduction(X, pca)\n",
    "    \n",
    "# 去掉空值\n",
    "yt = np.delete(y, np.where(~np.isnan(reduced_trainX).any(axis=1) == False))\n",
    "PACt = np.delete(PAC, np.where(~np.isnan(reduced_trainX).any(axis=1) == False))\n",
    "reduced_trainX = reduced_trainX[~np.isnan(reduced_trainX).any(axis=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储中间结果\n",
    "tX_df = pd.DataFrame(reduced_trainX)\n",
    "tX_df['Y'] = yt\n",
    "tX_df['PAC'] = PACt\n",
    "tX_df.to_csv('Process/trainData2017.csv', index = False, header = True)\n",
    "# 释放内存\n",
    "X = []\n",
    "X_ = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coarse Tuning + Delcate Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a sample\n",
    "tree_params = {'objective': 'reg:linear', 'booster': 'gbtree', 'random_state': 42}\n",
    "cv_params = {'eta': np.arange(0.1, 10, 3),\n",
    "             'subsample': np.arange(0.5, 1, 0.2),\n",
    "            'lambda': np.arange(1, 100, 30),\n",
    "             'min_child_weight': np.arange(1, 100, 30)}\n",
    "model = xgb.XGBRegressor(**tree_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='r2', cv=4, verbose=1)\n",
    "optimized_GBM.fit(reduced_trainX, np.log(yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print('参数的最佳取值：{0}'.format(optimized_GBM.best_params_))\n",
    "print('最佳模型得分:{0}'.format(optimized_GBM.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### iterate the tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters in our experiments\n",
    "# PCA = 21\n",
    "opt_params = {'objective': 'reg:linear', 'booster': 'gbtree', 'random_state': 42,\n",
    "              'subsample': 0.85, 'eta': 0.09, 'lambda': 0.1, 'min_child_weight': 75}\n",
    "opt_model = xgb.XGBRegressor(**opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入2018年数据\n",
    "data1 = pd.read_csv('Data/2018_features.csv')\n",
    "data1 = data1.dropna()\n",
    "label1 = pd.read_csv('Data/PAC_GDP18.csv')\n",
    "data1 = pd.merge(data1, label1, how='left')\n",
    "X1 = []\n",
    "y1 = []\n",
    "PAC1 = []\n",
    "for i in data1.index:\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    #try:\n",
    "    x_i = [float(x) for x in re.split(r', |\\[|\\]', data1['features'].loc[i]) if len(x) > 0]\n",
    "    x_i = np.array(x_i).reshape(-1, 4096)\n",
    "    X1.append(x_i)\n",
    "    y1.append(data1['GDP'].loc[i])\n",
    "    PAC1.append(data1['PAC'].loc[i])\n",
    "    #except:\n",
    "       # print('error')\n",
    "zeros1 = [X1.index(x) for x in X1 if x.shape[0] == 0]\n",
    "X1 = np.delete(np.array(X1), zeros1)\n",
    "y1 = np.delete(np.array(y1), zeros1)\n",
    "PAC1 = np.delete(np.array(PAC1), zeros1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降维\n",
    "reduced_validX = dimensional_reduction(X1, pca)\n",
    "    \n",
    "# 去掉空值\n",
    "y1v = np.delete(y1, np.where(~np.isnan(reduced_validX).any(axis=1) == False))\n",
    "PAC1v = np.delete(PAC1, np.where(~np.isnan(reduced_validX).any(axis=1) == False))\n",
    "reduced_validX = reduced_validX[~np.isnan(reduced_validX).any(axis=1), :]\n",
    "\n",
    "# 存储结果\n",
    "vX_df = pd.DataFrame(reduced_validX)\n",
    "vX_df['Y'] = y1v\n",
    "vX_df['PAC'] = PAC1v\n",
    "vX_df.to_csv('Process/validData2018.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model.fit(reduced_trianX, np.log(yt))\n",
    "y_pred = opt_model.predict(reduced_validX)\n",
    "r2_score(np.log(y1v), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'pred_y': np.exp(y_pred), 'y': y1v, 'PAC': PAC1v})\n",
    "pred_df.to_csv('Result/pred2018_PCA{}_XGB.csv'.format(pca_opt), header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
